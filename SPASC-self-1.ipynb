{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.214</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.379</td>\n",
       "      <td>96</td>\n",
       "      <td>583</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.928</td>\n",
       "      <td>20.432</td>\n",
       "      <td>213</td>\n",
       "      <td>1655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.828</td>\n",
       "      <td>1.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.047</td>\n",
       "      <td>54</td>\n",
       "      <td>768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.565</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4596</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4597</td>\n",
       "      <td>4597</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4598</td>\n",
       "      <td>4598</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4599</td>\n",
       "      <td>4599</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.632</td>\n",
       "      <td>11</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>4600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.616</td>\n",
       "      <td>13</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    X1    X2    X3   X4    X5    X6   X7    X8    X9  ...  \\\n",
       "0              0  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "1              1  0.05  0.30  0.40  0.0  0.10  0.05  0.0  0.05  0.10  ...   \n",
       "2              2  0.00  0.76  0.00  0.0  0.00  0.00  0.0  0.00  0.57  ...   \n",
       "3              3  0.09  0.49  0.59  0.0  0.39  0.19  0.0  0.00  0.09  ...   \n",
       "4              4  0.00  0.00  0.78  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "...          ...   ...   ...   ...  ...   ...   ...  ...   ...   ...  ...   \n",
       "4596        4596  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "4597        4597  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "4598        4598  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "4599        4599  0.00  0.00  0.26  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "4600        4600  0.00  0.23  0.00  0.0  0.00  0.23  0.0  0.46  0.00  ...   \n",
       "\n",
       "        X49    X50  X51    X52    X53    X54     X55  X56   X57  Y  \n",
       "0     0.000  0.000  0.0  0.000  0.000  0.000   1.214    4    17  0  \n",
       "1     0.000  0.036  0.0  0.054  0.118  0.000   2.379   96   583  1  \n",
       "2     0.000  0.099  0.0  0.232  0.066  0.928  20.432  213  1655  1  \n",
       "3     0.765  0.037  0.0  5.828  1.308  0.000   6.047   54   768  1  \n",
       "4     0.000  0.401  0.0  0.133  0.000  0.000   1.565    4    36  0  \n",
       "...     ...    ...  ...    ...    ...    ...     ...  ...   ... ..  \n",
       "4596  0.000  0.000  0.0  0.000  0.000  0.000   1.000    1     5  0  \n",
       "4597  0.000  0.000  0.0  0.000  0.000  0.000   1.000    1     4  0  \n",
       "4598  0.000  0.000  0.0  0.000  0.000  0.000   1.500    9    30  0  \n",
       "4599  0.036  0.109  0.0  0.000  0.036  0.000   1.632   11   307  0  \n",
       "4600  0.063  0.063  0.0  0.159  0.000  0.000   1.616   13   173  0  \n",
       "\n",
       "[4601 rows x 59 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'datasets/spambase.csv'\n",
    "data = pd.read_csv(dataset_name)\n",
    "THRESHOLD=0.95\n",
    "Q=5\n",
    "BETA=0.1\n",
    "pool={}\n",
    "# data = data.sample(frac=1).reset_index(drop=True)\n",
    "# data = data.apply(pd.to_numeric).fillna(0)\n",
    "# data = data.iloc[:, 1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "import numpy as np\n",
    "\n",
    "colors = 10 * [\"g\", \"r\", \"c\", \"b\", \"k\"]\n",
    "\n",
    "\n",
    "class K_Means:\n",
    "    def __init__(self, k=5, tol=0.001, max_iter=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def find_second_smallest(self, numbers):\n",
    "        m1, m2 = float('inf'), float('inf')\n",
    "        for x in numbers:\n",
    "            if x <= m1:\n",
    "                m1, m2 = x, m1\n",
    "            elif x < m2:\n",
    "                m2 = x\n",
    "        return m2\n",
    "    \n",
    "    def find_max_labels_in_cluster(self, cluster):\n",
    "        #print(\"Cluster: \" + str(cluster))\n",
    "        labels = []\n",
    "        for arr in cluster:\n",
    "            labels.append(arr[-1])\n",
    "        return max(labels, key=labels.count)\n",
    "\n",
    "    def fit(self, data):\n",
    "\n",
    "        self.centroids = {}\n",
    "        self.max_labels = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i][:-1]\n",
    "            #print(self.centroids[i].shape)\n",
    "            self.max_labels[i] = -9\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "            \n",
    "            for featureset in data:\n",
    "                #print(self.max_iter,self.max_labels)\n",
    "                distances = [np.linalg.norm(featureset[:-1] - self.centroids[centroid]) for centroid in self.centroids]#Euclidean distance                \n",
    "                classification = distances.index(min(distances))#Initial cluster\n",
    "                \n",
    "                if(featureset[-1] != -99):#Has a label\n",
    "                    if(self.max_labels[classification] == -9):#No labelled data is assigned to the cluster yet\n",
    "                        self.classifications[classification].append(featureset)\n",
    "                        self.max_labels[classification] = featureset[-1]\n",
    "                    elif(self.max_labels[classification] == featureset[-1]):#Max label matches then assign to that cluster\n",
    "                        self.classifications[classification].append(featureset)\n",
    "                        self.max_labels[classification] = self.find_max_labels_in_cluster(self.classifications[classification])\n",
    "                    else:#Assign to second nearst cluster with label equal to featureset[-1]\n",
    "                        labels_match = {}\n",
    "                        matched_centroids = {}\n",
    "                        \n",
    "                        for m in self.max_labels:\n",
    "                            if self.max_labels[m] == featureset[-1]:\n",
    "                                labels_match[m] = self.max_labels[m]\n",
    "                                matched_centroids[m] = self.centroids[m]\n",
    "                        \n",
    "                        if len(labels_match) == 0:#Empty. Assign to nearest cluster\n",
    "                            self.classifications[classification].append(featureset)\n",
    "                            self.max_labels[classification] = self.find_max_labels_in_cluster(self.classifications[classification])\n",
    "                        else:\n",
    "                            distances1 = [np.linalg.norm(featureset[:-1] - matched_centroids[centroid]) for centroid in matched_centroids]#Euclidean distance\n",
    "                            classification = distances1.index(min(distances1))\n",
    "                            self.classifications[classification].append(featureset)\n",
    "                            self.max_labels[classification] = self.find_max_labels_in_cluster(self.classifications[classification])\n",
    "\n",
    "                else:#Unlabelled\n",
    "                    self.classifications[classification].append(featureset)\n",
    "            print(\"MAX LABELS: \" + str(self.max_labels))\n",
    "            #print(\"Classifications: \" + str(self.classifications))\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "            #print(\"AAAA\")\n",
    "            \n",
    "                \n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification], axis=0)\n",
    "                self.centroids[classification]=np.delete(self.centroids[classification],-1)\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids:\n",
    "                \n",
    "                #print(\"AAA\")\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                #print(original_centroid.shape)\n",
    "                #print(current_centroid.shape)\n",
    "                if np.sum((current_centroid - original_centroid) / original_centroid * 100.0) > self.tol:\n",
    "                    optimized = False\n",
    "\n",
    "            if optimized:\n",
    "                break\n",
    "\n",
    "    def predict_first(self, data,mapping):\n",
    "        distances = [np.linalg.norm(data - self.centroids[centroid][:-1]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "    \n",
    "    def predict(self,data,mapping):\n",
    "        #print(data.shape)\n",
    "        #print(self.centroids[0].shape)\n",
    "        dist={}\n",
    "        for x in mapping.keys():\n",
    "            if(mapping[x]!=-9):\n",
    "                dist[x]=np.linalg.norm(data - self.centroids[x])\n",
    "    #if (len(sorted(dist.items(), key = lambda kv:(kv[1], kv[0])))==1):\n",
    "        #return sorted(dist.items(), key = lambda kv:(kv[1], kv[0]))[0]\n",
    "        return sorted(dist.items(), key = lambda kv:(kv[1], kv[0]))[0]\n",
    "    def map_funct(self):\n",
    "        return self.max_labels\n",
    "    def update(self, new_data, delta):\n",
    "        \n",
    "        for featureset in new_data:\n",
    "            \n",
    "            distances = [np.linalg.norm(featureset - self.centroids[centroid]) for centroid in self.centroids]\n",
    "            classification = distances.index(min(distances))\n",
    "            self.classifications[classification].append(featureset)\n",
    "            self.centroids[classification] = np.average(self.classifications[classification], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(data,Q,batch_split):\n",
    "    #print(data)\n",
    "    full_batch=data.iloc[:,:]\n",
    "    full_batch.iloc[batch_split:, -1] = -99 #Mark unlabelled data's label as \"-1\"\n",
    "    #print(full_batch)\n",
    "    kmeans = K_Means()\n",
    "    kmeans.fit(np.array(np.array(full_batch)))\n",
    "    mapping=kmeans.map_funct()\n",
    "    '''labelled_batch=data.iloc[:batch_split,:]\n",
    "    pred=[]\n",
    "    for i in range(len(labelled_batch)):\n",
    "        pred.append(kmeans.predict_first(np.array(data.iloc[0:len(labelled_batch),:-1])[i]))\n",
    "    labelled_batch['pred']=pred\n",
    "    df=labelled_batch.iloc[:,-2:]\n",
    "    a=df.groupby(['pred']).agg(lambda x:x.value_counts().index[0])\n",
    "    mapping={}\n",
    "    b=a.reset_index()\n",
    "    for i in range(0,len(b['pred'])):\n",
    "        mapping[b['pred'][i]]=b.iloc[:,-1][i]\n",
    "    print(mapping)'''\n",
    "    return kmeans.centroids,mapping,kmeans.classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pool(labelled_batch,batch_no,full_batch):\n",
    "    batch_simi={}\n",
    "    for classifier_name in pool:\n",
    "        similarity=find_accuracy(classifier_name,labelled_batch)\n",
    "        batch_simi[classifier_name]=similarity\n",
    "        \n",
    "    max_similarity_key=max(batch_simi, key=batch_simi.get)\n",
    "    max_similarity = batch_simi[max_similarity_key]\n",
    "    print(batch_simi)\n",
    "    model,mapping,classes=create_classifier(batch,Q,batch_split)\n",
    "    if(max_similarity>THRESHOLD or len(pool)>=max_c):\n",
    "        model_file=open(max_similarity_key,'wb')\n",
    "        map_file=open(max_similarity_key.split(\".\")[0],'wb')\n",
    "        no=str(str(max_similarity_key.split(\"Batch\")[1]).split(\".\")[0])\n",
    "        class_file=open(\"Class\"+str(no),'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "        pool[max_similarity_key]=1\n",
    "        '''model_file=open(max_similarity_key,'rb')\n",
    "        map_file=open(max_similarity_key.split(\".\")[0],'rb')\n",
    "        no=str(str(max_similarity_key.split(\"Batch\")[1]).split(\".\")[0])\n",
    "        class_file=open(\"Class\"+str(no),'rb')\n",
    "        km=K_Means()\n",
    "        km.centroids=pickle.load(model_file)\n",
    "        km.classifications=pickle.load(class_file)\n",
    "        km.update(labelled_batch.iloc[:,-1],9)\n",
    "        model_file=open(max_similarity_key,'wb')\n",
    "        class_file=open(\"Class\"+str(no),'wb')\n",
    "        pickle.dump(km.classifications,class_file)\n",
    "        pickle.dump(km.centroids,model_file)'''\n",
    "        \n",
    "    else:\n",
    "        model_name=\"Batch\"+str(batch_no)+\".sav\"\n",
    "        map_name=\"Batch\"+str(batch_no)\n",
    "        class_name=\"Class\"+str(batch_no)\n",
    "        model_file=open(model_name,'wb')\n",
    "        map_file=open(map_name,'wb')\n",
    "        class_file=open(class_name,'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "        pool[model_name]=1\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(classifier_name,labelled_batch):\n",
    "    s=0\n",
    "    model_file=open(classifier_name,'rb')\n",
    "    map_file=open(classifier_name.split(\".\")[0],'rb')\n",
    "    model=pickle.load(model_file)\n",
    "    mapping=pickle.load(map_file)\n",
    "    no=str(str(classifier_name.split(\"Batch\")[1]).split(\".\")[0])\n",
    "    class_file=open(\"Class\"+str(no),'rb')\n",
    "    classes=pickle.load(class_file)\n",
    "    km=K_Means()\n",
    "    km.classifications=classes\n",
    "    km.centroids=model\n",
    "    for i in range(len(labelled_batch)):\n",
    "        current_row=labelled_batch.iloc[i:i+1,:-1]\n",
    "        current_label=labelled_batch.iloc[i:i+1,-1]\n",
    "        pred=km.predict(current_row,mapping)\n",
    "        final_pred=mapping[pred[0]]\n",
    "        if(final_pred==current_label[i]):\n",
    "            s+=1\n",
    "    return((s)/len(labelled_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support,roc_auc_score, accuracy_score, f1_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dict1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-6ccce5676980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmap_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Batch\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mclass_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Class\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-40385220ac4f>\u001b[0m in \u001b[0;36mcreate_classifier\u001b[0;34m(data, Q, batch_split)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#print(full_batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK_Means\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmapping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_funct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     '''labelled_batch=data.iloc[:batch_split,:]\n",
      "\u001b[0;32m<ipython-input-32-f463ce7e7d2d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                 \u001b[0mmatched_centroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcentroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m#Empty. Assign to nearest cluster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifications\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_max_labels_in_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifications\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dict1' is not defined"
     ]
    }
   ],
   "source": [
    "pool={}\n",
    "batch_no=1\n",
    "stream_size=int(100)\n",
    "skip=0\n",
    "batch_split = int(0.2 * stream_size)\n",
    "max_c=10\n",
    "acc=[]\n",
    "s=0\n",
    "full_pred=[]\n",
    "full_true=[]\n",
    "#for j in range(0,6):\n",
    "for j in range(0,int(len(data)/stream_size)):\n",
    "    print(\"Batch\",batch_no)\n",
    "    batch = pd.read_csv(dataset_name, nrows = stream_size, skiprows=skip)\n",
    "    \n",
    "    if(j==0):\n",
    "        model_name=\"Batch\"+str(batch_no)+\".sav\"#Initial classifier\n",
    "        map_name=\"Batch\"+str(batch_no)\n",
    "        class_name=\"Class\"+str(batch_no)\n",
    "        model,mapping,classes=create_classifier(batch,Q,batch_split)\n",
    "        pool[model_name]=1\n",
    "        model_file=open(model_name,'wb')\n",
    "        map_file=open(map_name,'wb')\n",
    "        class_file=open(class_name,'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "    else:\n",
    "        #acc=[]\n",
    "        s=0\n",
    "        labelled_batch=batch.iloc[:batch_split,:]\n",
    "        pred_unlabelled=[]\n",
    "        true_unlabelled=[]\n",
    "        for i in range(len(labelled_batch)):\n",
    "            current_row=labelled_batch.iloc[i:i+1,:-1]\n",
    "            current_label=labelled_batch.iloc[i:i+1,-1]\n",
    "            Keymax = max(pool, key=pool.get)\n",
    "            model_file=open(Keymax,'rb')\n",
    "            map_file=open(Keymax.split(\".\")[0],'rb')\n",
    "            no=str(str(Keymax.split(\"Batch\")[1]).split(\".\")[0])\n",
    "            class_file=open(\"Class\"+str(no),'rb')\n",
    "            model=pickle.load(model_file)\n",
    "            mapping=pickle.load(map_file)\n",
    "            classes=pickle.load(class_file)\n",
    "            km=K_Means()\n",
    "            km.centroids=model\n",
    "            km.classifications=classes\n",
    "            pred=km.predict(current_row,mapping)\n",
    "            final_pred=mapping[pred[0]]\n",
    "            pred_unlabelled.append(final_pred)\n",
    "            true_unlabelled.append(current_label[i])\n",
    "            full_pred.append(final_pred)\n",
    "            full_true.append(current_label[i])\n",
    "            for classifier_name in pool:            \n",
    "                model_file=open(classifier_name,'rb')\n",
    "                map_file=open(classifier_name.split(\".\")[0],'rb')\n",
    "                no=str(str(classifier_name.split(\"Batch\")[1]).split(\".\")[0])\n",
    "                class_file=open(\"Class\"+str(no),'rb')\n",
    "                model=pickle.load(model_file)\n",
    "                mapping=pickle.load(map_file)\n",
    "                classes=pickle.load(class_file)\n",
    "                km=K_Means()\n",
    "                km.centroids=model\n",
    "                km.classifications=classes\n",
    "                pred=km.predict(current_row,mapping)\n",
    "                final_pred=mapping[pred[0]]\n",
    "                if(final_pred!=current_label[i]):\n",
    "                    pool[classifier_name]=pool[classifier_name]*(BETA**1)#Reduce weight otherwise\n",
    "        unlabelled_batch=batch.iloc[batch_split:,:]\n",
    "        for i in range(len(unlabelled_batch)):\n",
    "            current_row=unlabelled_batch.iloc[i:i+1,:-1]\n",
    "            current_label=unlabelled_batch.iloc[i:i+1,-1]\n",
    "            Keymax = max(pool, key=pool.get)\n",
    "            model_file=open(Keymax,'rb')\n",
    "            map_file=open(Keymax.split(\".\")[0],'rb')\n",
    "            no=str(str(Keymax.split(\"Batch\")[1]).split(\".\")[0])\n",
    "            class_file=open(\"Class\"+str(no),'rb')\n",
    "            model=pickle.load(model_file)\n",
    "            mapping=pickle.load(map_file)\n",
    "            classes=pickle.load(class_file)\n",
    "            km=K_Means()\n",
    "            km.centroids=model\n",
    "            km.classifications=classes\n",
    "            pred=km.predict(current_row,mapping)\n",
    "            final_pred=mapping[pred[0]]\n",
    "            pred_unlabelled.append(final_pred)\n",
    "            true_unlabelled.append(current_label[i+batch_split])\n",
    "            full_pred.append(final_pred)\n",
    "            full_true.append(current_label[i+batch_split])\n",
    "            #if(final_pred==current_label[i+batch_split]):\n",
    "              #  s+=1\n",
    "        pool=update_pool(labelled_batch,batch_no,batch)#Update pool\n",
    "        #print(true_unlabelled)\n",
    "        #print(pred_unlabelled)\n",
    "        print(classification_report(true_unlabelled,pred_unlabelled))\n",
    "    #print(acc)\n",
    "    skip += stream_size\n",
    "    batch_no += 1\n",
    "acc1 = accuracy_score(full_true,full_pred)\n",
    "f11 = f1_score(full_true,full_pred,average=None)\n",
    "re1 = recall_score(full_true, full_pred, average = None)\n",
    "pre1 = precision_score(full_true, full_pred, average = None)\n",
    "rep1 = classification_report(full_true, full_pred)\n",
    "final_df = pd.DataFrame(columns = ['Dataset', 'Labelled percent', 'Distance used',  'Accuracy', 'F1', 'Recall', 'Precision', 'Report'])\n",
    "final_df.loc[len(final_df)] = {'Dataset':\"Spambase\", 'Labelled percent':'20',\n",
    "                                    'Distance used':'Euclidean', 'Accuracy': acc1, 'F1': f11,\n",
    "                                    'Recall': re1, 'Precision': pre1, 'Report': rep1} \n",
    "final_df.to_csv('SPASC_results_' + 'Spambase' +  '_ossilh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
