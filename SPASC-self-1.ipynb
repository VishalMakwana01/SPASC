{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>X51</th>\n",
       "      <th>X52</th>\n",
       "      <th>X53</th>\n",
       "      <th>X54</th>\n",
       "      <th>X55</th>\n",
       "      <th>X56</th>\n",
       "      <th>X57</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.214</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.379</td>\n",
       "      <td>96</td>\n",
       "      <td>583</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.928</td>\n",
       "      <td>20.432</td>\n",
       "      <td>213</td>\n",
       "      <td>1655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.828</td>\n",
       "      <td>1.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.047</td>\n",
       "      <td>54</td>\n",
       "      <td>768</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.565</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4596</td>\n",
       "      <td>4596</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4597</td>\n",
       "      <td>4597</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4598</td>\n",
       "      <td>4598</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4599</td>\n",
       "      <td>4599</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.632</td>\n",
       "      <td>11</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>4600</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.159</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.616</td>\n",
       "      <td>13</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows Ã— 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0    X1    X2    X3   X4    X5    X6   X7    X8    X9  ...  \\\n",
       "0              0  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "1              1  0.05  0.30  0.40  0.0  0.10  0.05  0.0  0.05  0.10  ...   \n",
       "2              2  0.00  0.76  0.00  0.0  0.00  0.00  0.0  0.00  0.57  ...   \n",
       "3              3  0.09  0.49  0.59  0.0  0.39  0.19  0.0  0.00  0.09  ...   \n",
       "4              4  0.00  0.00  0.78  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "...          ...   ...   ...   ...  ...   ...   ...  ...   ...   ...  ...   \n",
       "4596        4596  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "4597        4597  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "4598        4598  0.00  0.00  0.00  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "4599        4599  0.00  0.00  0.26  0.0  0.00  0.00  0.0  0.00  0.00  ...   \n",
       "4600        4600  0.00  0.23  0.00  0.0  0.00  0.23  0.0  0.46  0.00  ...   \n",
       "\n",
       "        X49    X50  X51    X52    X53    X54     X55  X56   X57  Y  \n",
       "0     0.000  0.000  0.0  0.000  0.000  0.000   1.214    4    17  0  \n",
       "1     0.000  0.036  0.0  0.054  0.118  0.000   2.379   96   583  1  \n",
       "2     0.000  0.099  0.0  0.232  0.066  0.928  20.432  213  1655  1  \n",
       "3     0.765  0.037  0.0  5.828  1.308  0.000   6.047   54   768  1  \n",
       "4     0.000  0.401  0.0  0.133  0.000  0.000   1.565    4    36  0  \n",
       "...     ...    ...  ...    ...    ...    ...     ...  ...   ... ..  \n",
       "4596  0.000  0.000  0.0  0.000  0.000  0.000   1.000    1     5  0  \n",
       "4597  0.000  0.000  0.0  0.000  0.000  0.000   1.000    1     4  0  \n",
       "4598  0.000  0.000  0.0  0.000  0.000  0.000   1.500    9    30  0  \n",
       "4599  0.036  0.109  0.0  0.000  0.036  0.000   1.632   11   307  0  \n",
       "4600  0.063  0.063  0.0  0.159  0.000  0.000   1.616   13   173  0  \n",
       "\n",
       "[4601 rows x 59 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset_name = 'datasets/spambase.csv'\n",
    "dataset_name = 'datasets/spambase.csv'\n",
    "data = pd.read_csv(dataset_name)\n",
    "THRESHOLD=0.95\n",
    "Q=5\n",
    "BETA=0.1\n",
    "pool={}\n",
    "# data = data.sample(frac=1).reset_index(drop=True)\n",
    "# data = data.apply(pd.to_numeric).fillna(0)\n",
    "# data = data.iloc[:, 1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "import numpy as np\n",
    "\n",
    "colors = 10 * [\"g\", \"r\", \"c\", \"b\", \"k\"]\n",
    "numb_of_instances = {}\n",
    "\n",
    "\n",
    "class K_Means:\n",
    "    def __init__(self, k=5, tol=0.001, max_iter=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    '''def find_second_smallest(self, numbers):\n",
    "        m1, m2 = float('inf'), float('inf')\n",
    "        for x in numbers:\n",
    "            if x <= m1:\n",
    "                m1, m2 = x, m1\n",
    "            elif x < m2:\n",
    "                m2 = x\n",
    "        return m2'''\n",
    "    \n",
    "    def find_max_labels_in_cluster(self, cluster):\n",
    "        #print(\"Cluster: \" + str(cluster))\n",
    "        labels = []\n",
    "        for arr in cluster:\n",
    "            labels.append(arr[-1])\n",
    "        return max(labels, key=labels.count)\n",
    "\n",
    "    def fit(self, data):\n",
    "\n",
    "        self.centroids = {}\n",
    "        self.max_labels = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i][:-1]\n",
    "            #print(self.centroids[i].shape)\n",
    "            self.max_labels[i] = -9\n",
    "            numb_of_instances[i] = 0\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "            \n",
    "            for featureset in data:\n",
    "                #print(self.max_iter,self.max_labels)\n",
    "                distances = [np.linalg.norm(featureset[:-1] - self.centroids[centroid]) for centroid in self.centroids]#Euclidean distance                \n",
    "                classification = distances.index(min(distances))#Initial cluster\n",
    "                \n",
    "                if(featureset[-1] != -99):#Has a label\n",
    "                    if(self.max_labels[classification] == -9):#No labelled data is assigned to the cluster yet\n",
    "                        self.classifications[classification].append(featureset)\n",
    "                        self.max_labels[classification] = featureset[-1]\n",
    "                        numb_of_instances[classification] += 1\n",
    "                    elif(self.max_labels[classification] == featureset[-1]):#Max label matches then assign to that cluster\n",
    "                        self.classifications[classification].append(featureset)\n",
    "                        self.max_labels[classification] = self.find_max_labels_in_cluster(self.classifications[classification])\n",
    "                        numb_of_instances[classification] += 1\n",
    "                    else:#Assign to second nearst cluster with label equal to featureset[-1]\n",
    "                        labels_match = {}\n",
    "                        matched_centroids = {}\n",
    "                        \n",
    "                        for m in self.max_labels:\n",
    "                            if self.max_labels[m] == featureset[-1]:\n",
    "                                labels_match[m] = self.max_labels[m]\n",
    "                                matched_centroids[m] = self.centroids[m]\n",
    "                        \n",
    "                        if len(labels_match) == 0:#Empty. Assign to nearest cluster\n",
    "                            self.classifications[classification].append(featureset)\n",
    "                            self.max_labels[classification] = self.find_max_labels_in_cluster(self.classifications[classification])\n",
    "                            numb_of_instances[classification] += 1\n",
    "                        else:\n",
    "                            distances1 = [np.linalg.norm(featureset[:-1] - matched_centroids[centroid]) for centroid in matched_centroids]#Euclidean distance\n",
    "                            classification = distances1.index(min(distances1))\n",
    "                            self.classifications[classification].append(featureset)\n",
    "                            self.max_labels[classification] = self.find_max_labels_in_cluster(self.classifications[classification])\n",
    "                            numb_of_instances[classification] += 1\n",
    "                else:#Unlabelled\n",
    "                    self.classifications[classification].append(featureset)\n",
    "                    numb_of_instances[classification] += 1\n",
    "            print(\"MAX LABELS: \" + str(self.max_labels))\n",
    "            print(\"Max instances: \" + str(numb_of_instances))\n",
    "            #print(\"Classifications: \" + str(self.classifications))\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "            #print(\"AAAA\")\n",
    "            \n",
    "                \n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification], axis=0)\n",
    "                self.centroids[classification]=np.delete(self.centroids[classification],-1)\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids:\n",
    "                \n",
    "                #print(\"AAA\")\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                #print(original_centroid.shape)\n",
    "                #print(current_centroid.shape)\n",
    "                if np.sum((current_centroid - original_centroid) / original_centroid * 100.0) > self.tol:\n",
    "                    optimized = False\n",
    "\n",
    "            if optimized:\n",
    "                break\n",
    "\n",
    "    def predict_first(self, data,mapping):\n",
    "        distances = [np.linalg.norm(data - self.centroids[centroid][:-1]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "    \n",
    "    def predict(self,data,mapping):\n",
    "        #print(data.shape)\n",
    "        #print(self.centroids[0].shape)\n",
    "        dist={}\n",
    "        for x in mapping.keys():\n",
    "            if(mapping[x]!=-9):\n",
    "                dist[x]=np.linalg.norm(data - self.centroids[x])\n",
    "    #if (len(sorted(dist.items(), key = lambda kv:(kv[1], kv[0])))==1):\n",
    "        #return sorted(dist.items(), key = lambda kv:(kv[1], kv[0]))[0]\n",
    "        return sorted(dist.items(), key = lambda kv:(kv[1], kv[0]))[0]\n",
    "    def map_funct(self):\n",
    "        return self.max_labels\n",
    "    def update(self, new_data, delta):\n",
    "        \n",
    "        for featureset in new_data:\n",
    "            \n",
    "            distances = [np.linalg.norm(featureset - self.centroids[centroid]) for centroid in self.centroids]\n",
    "            classification = distances.index(min(distances))\n",
    "            self.classifications[classification].append(featureset)\n",
    "            self.centroids[classification] = np.average(self.classifications[classification], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(data,Q,batch_split):\n",
    "    #print(data)\n",
    "    full_batch=data.iloc[:,:]\n",
    "    full_batch.iloc[batch_split:, -1] = -99 #Mark unlabelled data's label as \"-1\"\n",
    "    #print(full_batch)\n",
    "    kmeans = K_Means()\n",
    "    kmeans.fit(np.array(np.array(full_batch)))\n",
    "    mapping=kmeans.map_funct()\n",
    "    '''labelled_batch=data.iloc[:batch_split,:]\n",
    "    pred=[]\n",
    "    for i in range(len(labelled_batch)):\n",
    "        pred.append(kmeans.predict_first(np.array(data.iloc[0:len(labelled_batch),:-1])[i]))\n",
    "    labelled_batch['pred']=pred\n",
    "    df=labelled_batch.iloc[:,-2:]\n",
    "    a=df.groupby(['pred']).agg(lambda x:x.value_counts().index[0])\n",
    "    mapping={}\n",
    "    b=a.reset_index()\n",
    "    for i in range(0,len(b['pred'])):\n",
    "        mapping[b['pred'][i]]=b.iloc[:,-1][i]\n",
    "    print(mapping)'''\n",
    "    return kmeans.centroids,mapping,kmeans.classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pool(labelled_batch,batch_no,full_batch):\n",
    "    batch_simi={}\n",
    "    for classifier_name in pool:\n",
    "        similarity=find_accuracy(classifier_name,labelled_batch)\n",
    "        batch_simi[classifier_name]=similarity\n",
    "        \n",
    "    max_similarity_key=max(batch_simi, key=batch_simi.get)\n",
    "    max_similarity = batch_simi[max_similarity_key]\n",
    "    print(batch_simi)\n",
    "    model,mapping,classes=create_classifier(batch,Q,batch_split)\n",
    "    if(max_similarity>THRESHOLD or len(pool)>=max_c):\n",
    "        model_file=open(max_similarity_key,'wb')\n",
    "        map_file=open(max_similarity_key.split(\".\")[0],'wb')\n",
    "        no=str(str(max_similarity_key.split(\"Batch\")[1]).split(\".\")[0])\n",
    "        class_file=open(\"Class\"+str(no),'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "        pool[max_similarity_key]=1\n",
    "        '''model_file=open(max_similarity_key,'rb')\n",
    "        map_file=open(max_similarity_key.split(\".\")[0],'rb')\n",
    "        no=str(str(max_similarity_key.split(\"Batch\")[1]).split(\".\")[0])\n",
    "        class_file=open(\"Class\"+str(no),'rb')\n",
    "        km=K_Means()\n",
    "        km.centroids=pickle.load(model_file)\n",
    "        km.classifications=pickle.load(class_file)\n",
    "        km.update(labelled_batch.iloc[:,-1],9)\n",
    "        model_file=open(max_similarity_key,'wb')\n",
    "        class_file=open(\"Class\"+str(no),'wb')\n",
    "        pickle.dump(km.classifications,class_file)\n",
    "        pickle.dump(km.centroids,model_file)'''\n",
    "        \n",
    "    else:\n",
    "        model_name=\"Batch\"+str(batch_no)+\".sav\"\n",
    "        map_name=\"Batch\"+str(batch_no)\n",
    "        class_name=\"Class\"+str(batch_no)\n",
    "        model_file=open(model_name,'wb')\n",
    "        map_file=open(map_name,'wb')\n",
    "        class_file=open(class_name,'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "        pool[model_name]=1\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(classifier_name,labelled_batch):\n",
    "    s=0\n",
    "    model_file=open(classifier_name,'rb')\n",
    "    map_file=open(classifier_name.split(\".\")[0],'rb')\n",
    "    model=pickle.load(model_file)\n",
    "    mapping=pickle.load(map_file)\n",
    "    no=str(str(classifier_name.split(\"Batch\")[1]).split(\".\")[0])\n",
    "    class_file=open(\"Class\"+str(no),'rb')\n",
    "    classes=pickle.load(class_file)\n",
    "    km=K_Means()\n",
    "    km.classifications=classes\n",
    "    km.centroids=model\n",
    "    for i in range(len(labelled_batch)):\n",
    "        current_row=labelled_batch.iloc[i:i+1,:-1]\n",
    "        current_label=labelled_batch.iloc[i:i+1,-1]\n",
    "        pred=km.predict(current_row,mapping)\n",
    "        final_pred=mapping[pred[0]]\n",
    "        if(final_pred==current_label[i]):\n",
    "            s+=1\n",
    "    return((s)/len(labelled_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support,roc_auc_score, accuracy_score, f1_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "MAX LABELS: {0: 0.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 0.0}\n",
      "Max instances: {0: 18, 1: 9, 2: 2, 3: 7, 4: 64}\n",
      "Batch 2\n",
      "{'Batch1.sav': 0.75}\n",
      "MAX LABELS: {0: 1.0, 1: 1.0, 2: 1.0, 3: 0.0, 4: 0.0}\n",
      "Max instances: {0: 17, 1: 20, 2: 6, 3: 37, 4: 20}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.91      0.75        54\n",
      "           1       0.79      0.41      0.54        46\n",
      "\n",
      "    accuracy                           0.68       100\n",
      "   macro avg       0.72      0.66      0.65       100\n",
      "weighted avg       0.71      0.68      0.66       100\n",
      "\n",
      "Batch 3\n",
      "{'Batch1.sav': 0.75, 'Batch2.sav': 0.6}\n",
      "MAX LABELS: {0: 1.0, 1: 1.0, 2: 0.0, 3: 0.0, 4: 0.0}\n",
      "Max instances: {0: 9, 1: 25, 2: 24, 3: 14, 4: 28}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.87      0.67        52\n",
      "           1       0.59      0.21      0.31        48\n",
      "\n",
      "    accuracy                           0.55       100\n",
      "   macro avg       0.57      0.54      0.49       100\n",
      "weighted avg       0.56      0.55      0.49       100\n",
      "\n",
      "Batch 4\n",
      "{'Batch1.sav': 0.8, 'Batch2.sav': 0.8, 'Batch3.sav': 0.75}\n",
      "MAX LABELS: {0: 0.0, 1: 0.0, 2: 1.0, 3: 0.0, 4: 0.0}\n",
      "Max instances: {0: 28, 1: 13, 2: 34, 3: 15, 4: 10}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.93      0.85        71\n",
      "           1       0.67      0.34      0.45        29\n",
      "\n",
      "    accuracy                           0.76       100\n",
      "   macro avg       0.72      0.64      0.65       100\n",
      "weighted avg       0.74      0.76      0.73       100\n",
      "\n",
      "Batch 5\n",
      "{'Batch1.sav': 0.7, 'Batch2.sav': 0.65, 'Batch3.sav': 0.65, 'Batch4.sav': 0.7}\n",
      "MAX LABELS: {0: 0.0, 1: 0.0, 2: 0.0, 3: 0.0, 4: 0.0}\n",
      "Max instances: {0: 31, 1: 18, 2: 15, 3: 11, 4: 25}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.75      0.69        63\n",
      "           1       0.41      0.30      0.34        37\n",
      "\n",
      "    accuracy                           0.58       100\n",
      "   macro avg       0.53      0.52      0.52       100\n",
      "weighted avg       0.56      0.58      0.56       100\n",
      "\n",
      "Batch 6\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Batch5.sav'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b2c2618d9069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mcurrent_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munlabelled_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mKeymax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mmodel_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeymax\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mmap_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeymax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mno\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeymax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Batch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Batch5.sav'"
     ]
    }
   ],
   "source": [
    "pool={}\n",
    "batch_no=1\n",
    "stream_size=int(100)\n",
    "skip=0\n",
    "batch_split = int(0.2 * stream_size)\n",
    "max_c=10\n",
    "acc=[]\n",
    "s=0\n",
    "full_pred=[]\n",
    "full_true=[]\n",
    "final_df = pd.DataFrame(columns = ['Dataset', 'Labelled percent', 'Distance used','Batch No',  'Accuracy', 'F1', 'Recall', 'Precision', 'Report'])\n",
    "#for j in range(0,6):\n",
    "for j in range(0,int(len(data)/stream_size)):\n",
    "    print(\"Batch\",batch_no)\n",
    "    batch = pd.read_csv(dataset_name, nrows = stream_size, skiprows=skip)\n",
    "    \n",
    "    if(j==0):\n",
    "        model_name=\"Batch\"+str(batch_no)+\".sav\"#Initial classifier\n",
    "        map_name=\"Batch\"+str(batch_no)\n",
    "        class_name=\"Class\"+str(batch_no)\n",
    "        model,mapping,classes=create_classifier(batch,Q,batch_split)\n",
    "        pool[model_name]=1\n",
    "        model_file=open(model_name,'wb')\n",
    "        map_file=open(map_name,'wb')\n",
    "        class_file=open(class_name,'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "    else:\n",
    "        #acc=[]\n",
    "        s=0\n",
    "        labelled_batch=batch.iloc[:batch_split,:]\n",
    "        pred_unlabelled=[]\n",
    "        true_unlabelled=[]\n",
    "        for i in range(len(labelled_batch)):\n",
    "            current_row=labelled_batch.iloc[i:i+1,:-1]\n",
    "            current_label=labelled_batch.iloc[i:i+1,-1]\n",
    "            Keymax = max(pool, key=pool.get)\n",
    "            model_file=open(Keymax,'rb')\n",
    "            map_file=open(Keymax.split(\".\")[0],'rb')\n",
    "            no=str(str(Keymax.split(\"Batch\")[1]).split(\".\")[0])\n",
    "            class_file=open(\"Class\"+str(no),'rb')\n",
    "            model=pickle.load(model_file)\n",
    "            mapping=pickle.load(map_file)\n",
    "            classes=pickle.load(class_file)\n",
    "            km=K_Means()\n",
    "            km.centroids=model\n",
    "            km.classifications=classes\n",
    "            pred=km.predict(current_row,mapping)\n",
    "            final_pred=mapping[pred[0]]\n",
    "            pred_unlabelled.append(final_pred)\n",
    "            true_unlabelled.append(current_label[i])\n",
    "            full_pred.append(final_pred)\n",
    "            full_true.append(current_label[i])\n",
    "            for classifier_name in pool:            \n",
    "                model_file=open(classifier_name,'rb')\n",
    "                map_file=open(classifier_name.split(\".\")[0],'rb')\n",
    "                no=str(str(classifier_name.split(\"Batch\")[1]).split(\".\")[0])\n",
    "                class_file=open(\"Class\"+str(no),'rb')\n",
    "                model=pickle.load(model_file)\n",
    "                mapping=pickle.load(map_file)\n",
    "                classes=pickle.load(class_file)\n",
    "                km=K_Means()\n",
    "                km.centroids=model\n",
    "                km.classifications=classes\n",
    "                pred=km.predict(current_row,mapping)\n",
    "                final_pred=mapping[pred[0]]\n",
    "                if(final_pred!=current_label[i]):\n",
    "                    pool[classifier_name]=pool[classifier_name]*(BETA**1)#Reduce weight otherwise\n",
    "        unlabelled_batch=batch.iloc[batch_split:,:]\n",
    "        for i in range(len(unlabelled_batch)):\n",
    "            current_row=unlabelled_batch.iloc[i:i+1,:-1]\n",
    "            current_label=unlabelled_batch.iloc[i:i+1,-1]\n",
    "            Keymax = max(pool, key=pool.get)\n",
    "            model_file=open(Keymax,'rb')\n",
    "            map_file=open(Keymax.split(\".\")[0],'rb')\n",
    "            no=str(str(Keymax.split(\"Batch\")[1]).split(\".\")[0])\n",
    "            class_file=open(\"Class\"+str(no),'rb')\n",
    "            model=pickle.load(model_file)\n",
    "            mapping=pickle.load(map_file)\n",
    "            classes=pickle.load(class_file)\n",
    "            km=K_Means()\n",
    "            km.centroids=model\n",
    "            km.classifications=classes\n",
    "            pred=km.predict(current_row,mapping)\n",
    "            final_pred=mapping[pred[0]]\n",
    "            pred_unlabelled.append(final_pred)\n",
    "            true_unlabelled.append(current_label[i+batch_split])\n",
    "            full_pred.append(final_pred)\n",
    "            full_true.append(current_label[i+batch_split])\n",
    "            #if(final_pred==current_label[i+batch_split]):\n",
    "              #  s+=1\n",
    "        pool=update_pool(labelled_batch,batch_no,batch)#Update pool\n",
    "        #print(true_unlabelled)\n",
    "        #print(pred_unlabelled)\n",
    "        acc = accuracy_score(true_unlabelled,pred_unlabelled)\n",
    "        f1 = f1_score(true_unlabelled,pred_unlabelled,average=None)\n",
    "        re = recall_score(true_unlabelled, pred_unlabelled, average = None)\n",
    "        pre = precision_score(true_unlabelled, pred_unlabelled, average = None)\n",
    "        rep = classification_report(true_unlabelled, pred_unlabelled)\n",
    "\n",
    "        final_df.loc[len(final_df)] = {'Dataset':\"Spambase\", 'Labelled percent':'20',\n",
    "                                    'Distance used':'Euclidean','Batch No': batch_no, 'Accuracy': acc, 'F1': f1,\n",
    "                                    'Recall': re, 'Precision': pre, 'Report': rep} \n",
    "        print(classification_report(true_unlabelled,pred_unlabelled))\n",
    "    #print(acc)\n",
    "    skip += stream_size\n",
    "    batch_no += 1\n",
    "acc1 = accuracy_score(full_true,full_pred)\n",
    "f11 = f1_score(full_true,full_pred,average=None)\n",
    "re1 = recall_score(full_true, full_pred, average = None)\n",
    "pre1 = precision_score(full_true, full_pred, average = None)\n",
    "rep1 = classification_report(full_true, full_pred)\n",
    "\n",
    "final_df.loc[len(final_df)] = {'Dataset':\"Spambase\", 'Labelled percent':'20',\n",
    "                                    'Distance used':'Euclidean','Batch No': 'Full', 'Accuracy': acc1, 'F1': f11,\n",
    "                                    'Recall': re1, 'Precision': pre1, 'Report': rep1} \n",
    "final_df.to_csv('SPASC_results_' + 'Spambase' +  '_ossilh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
