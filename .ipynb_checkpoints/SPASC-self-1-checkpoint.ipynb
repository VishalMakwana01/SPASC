{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.utils import shuffle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'datasets/spambase.csv' does not exist: b'datasets/spambase.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-92d60e9554b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'datasets/spambase.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mTHRESHOLD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mBETA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'datasets/spambase.csv' does not exist: b'datasets/spambase.csv'"
     ]
    }
   ],
   "source": [
    "dataset_name = 'datasets/spambase.csv'\n",
    "data = pd.read_csv(dataset_name)\n",
    "THRESHOLD=0.95\n",
    "Q=5\n",
    "BETA=0.1\n",
    "pool={}\n",
    "# data = data.sample(frac=1).reset_index(drop=True)\n",
    "# data = data.apply(pd.to_numeric).fillna(0)\n",
    "# data = data.iloc[:, 1:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use('ggplot')\n",
    "import numpy as np\n",
    "\n",
    "colors = 10 * [\"g\", \"r\", \"c\", \"b\", \"k\"]\n",
    "\n",
    "\n",
    "class K_Means:\n",
    "    def __init__(self, k=5, tol=0.001, max_iter=300):\n",
    "        self.k = k\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "\n",
    "    def find_second_smallest(self, numbers):\n",
    "        m1, m2 = float('inf'), float('inf')\n",
    "        for x in numbers:\n",
    "            if x <= m1:\n",
    "                m1, m2 = x, m1\n",
    "            elif x < m2:\n",
    "                m2 = x\n",
    "        return m2\n",
    "    \n",
    "    def find_max_labels_in_cluster(self, cluster):\n",
    "        #print(\"Cluster: \" + str(cluster))\n",
    "        labels = []\n",
    "        for arr in cluster:\n",
    "            labels.append(arr[-1])\n",
    "        return max(labels, key=labels.count)\n",
    "\n",
    "    def fit(self, data):\n",
    "\n",
    "        self.centroids = {}\n",
    "        self.max_labels = {}\n",
    "\n",
    "        for i in range(self.k):\n",
    "            self.centroids[i] = data[i][:-1]\n",
    "            #print(self.centroids[i].shape)\n",
    "            self.max_labels[i] = -9\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "\n",
    "            for i in range(self.k):\n",
    "                self.classifications[i] = []\n",
    "            \n",
    "            for featureset in data:\n",
    "                #print(self.max_iter,self.max_labels)\n",
    "                distances = [np.linalg.norm(featureset[:-1] - self.centroids[centroid]) for centroid in self.centroids]#Euclidean distance                \n",
    "                classification = distances.index(min(distances))#Initial cluster\n",
    "                \n",
    "                if(featureset[-1] != -99):#Has a label\n",
    "                    if(self.max_labels[classification] == -9):#No labelled data is assigned to the cluster yet\n",
    "                        self.classifications[classification].append(featureset)\n",
    "                        self.max_labels[classification] = featureset[-1]\n",
    "                    elif(self.max_labels[classification] == featureset[-1]):#Max label matches then assign to that cluster\n",
    "                        self.classifications[classification].append(featureset)\n",
    "                        self.max_labels[classification] = self.find_max_labels_in_cluster(self.classifications[classification])\n",
    "                    else:#Assign to second nearst cluster\n",
    "                        classification = distances.index(self.find_second_smallest(distances))\n",
    "                        self.classifications[classification].append(featureset)\n",
    "                        self.max_labels[classification] = self.find_max_labels_in_cluster(self.classifications[classification])\n",
    "                else:#Unlabelled\n",
    "                    self.classifications[classification].append(featureset)\n",
    "            print(\"MAX LABELS: \" + str(self.max_labels))\n",
    "            #print(\"Classifications: \" + str(self.classifications))\n",
    "\n",
    "            prev_centroids = dict(self.centroids)\n",
    "            #print(\"AAAA\")\n",
    "            \n",
    "                \n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification], axis=0)\n",
    "                self.centroids[classification]=np.delete(self.centroids[classification],-1)\n",
    "            optimized = True\n",
    "\n",
    "            for c in self.centroids:\n",
    "                \n",
    "                #print(\"AAA\")\n",
    "                original_centroid = prev_centroids[c]\n",
    "                current_centroid = self.centroids[c]\n",
    "                #print(original_centroid.shape)\n",
    "                #print(current_centroid.shape)\n",
    "                if np.sum((current_centroid - original_centroid) / original_centroid * 100.0) > self.tol:\n",
    "                    optimized = False\n",
    "\n",
    "            if optimized:\n",
    "                break\n",
    "\n",
    "    def predict_first(self, data,mapping):\n",
    "        distances = [np.linalg.norm(data - self.centroids[centroid][:-1]) for centroid in self.centroids]\n",
    "        classification = distances.index(min(distances))\n",
    "        return classification\n",
    "    \n",
    "    def predict(self,data,mapping):\n",
    "        #print(data.shape)\n",
    "        #print(self.centroids[0].shape)\n",
    "        dist={}\n",
    "        for x in mapping.keys():\n",
    "            if(mapping[x]!=-9):\n",
    "                dist[x]=np.linalg.norm(data - self.centroids[x])\n",
    "    #if (len(sorted(dist.items(), key = lambda kv:(kv[1], kv[0])))==1):\n",
    "        #return sorted(dist.items(), key = lambda kv:(kv[1], kv[0]))[0]\n",
    "        return sorted(dist.items(), key = lambda kv:(kv[1], kv[0]))[0]\n",
    "    def map_funct(self):\n",
    "        return self.max_labels\n",
    "    def update(self, new_data, delta):\n",
    "        \n",
    "        for featureset in new_data:\n",
    "            \n",
    "            distances = [np.linalg.norm(featureset - self.centroids[centroid]) for centroid in self.centroids]\n",
    "            classification = distances.index(min(distances))\n",
    "            self.classifications[classification].append(featureset)\n",
    "            self.centroids[classification] = np.average(self.classifications[classification], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(data,Q,batch_split):\n",
    "    #print(data)\n",
    "    full_batch=data.iloc[:,:]\n",
    "    full_batch.iloc[batch_split:, -1] = -99 #Mark unlabelled data's label as \"-1\"\n",
    "    #print(full_batch)\n",
    "    kmeans = K_Means()\n",
    "    kmeans.fit(np.array(np.array(full_batch)))\n",
    "    mapping=kmeans.map_funct()\n",
    "    '''labelled_batch=data.iloc[:batch_split,:]\n",
    "    pred=[]\n",
    "    for i in range(len(labelled_batch)):\n",
    "        pred.append(kmeans.predict_first(np.array(data.iloc[0:len(labelled_batch),:-1])[i]))\n",
    "    labelled_batch['pred']=pred\n",
    "    df=labelled_batch.iloc[:,-2:]\n",
    "    a=df.groupby(['pred']).agg(lambda x:x.value_counts().index[0])\n",
    "    mapping={}\n",
    "    b=a.reset_index()\n",
    "    for i in range(0,len(b['pred'])):\n",
    "        mapping[b['pred'][i]]=b.iloc[:,-1][i]\n",
    "    print(mapping)'''\n",
    "    return kmeans.centroids,mapping,kmeans.classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_pool(labelled_batch,batch_no,full_batch):\n",
    "    batch_simi={}\n",
    "    for classifier_name in pool:\n",
    "        similarity=find_accuracy(classifier_name,labelled_batch)\n",
    "        batch_simi[classifier_name]=similarity\n",
    "        \n",
    "    max_similarity_key=max(batch_simi, key=batch_simi.get)\n",
    "    max_similarity = batch_simi[max_similarity_key]\n",
    "    print(batch_simi)\n",
    "    model,mapping,classes=create_classifier(batch,Q,batch_split)\n",
    "    if(max_similarity>THRESHOLD or len(pool)>=max_c):\n",
    "        model_file=open(max_similarity_key,'wb')\n",
    "        map_file=open(max_similarity_key.split(\".\")[0],'wb')\n",
    "        no=str(str(max_similarity_key.split(\"Batch\")[1]).split(\".\")[0])\n",
    "        class_file=open(\"Class\"+str(no),'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "        pool[max_similarity_key]=1\n",
    "        '''model_file=open(max_similarity_key,'rb')\n",
    "        map_file=open(max_similarity_key.split(\".\")[0],'rb')\n",
    "        no=str(str(max_similarity_key.split(\"Batch\")[1]).split(\".\")[0])\n",
    "        class_file=open(\"Class\"+str(no),'rb')\n",
    "        km=K_Means()\n",
    "        km.centroids=pickle.load(model_file)\n",
    "        km.classifications=pickle.load(class_file)\n",
    "        km.update(labelled_batch.iloc[:,-1],9)\n",
    "        model_file=open(max_similarity_key,'wb')\n",
    "        class_file=open(\"Class\"+str(no),'wb')\n",
    "        pickle.dump(km.classifications,class_file)\n",
    "        pickle.dump(km.centroids,model_file)'''\n",
    "        \n",
    "    else:\n",
    "        model_name=\"Batch\"+str(batch_no)+\".sav\"\n",
    "        map_name=\"Batch\"+str(batch_no)\n",
    "        class_name=\"Class\"+str(batch_no)\n",
    "        model_file=open(model_name,'wb')\n",
    "        map_file=open(map_name,'wb')\n",
    "        class_file=open(class_name,'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "        pool[model_name]=1\n",
    "    return pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_accuracy(classifier_name,labelled_batch):\n",
    "    s=0\n",
    "    model_file=open(classifier_name,'rb')\n",
    "    map_file=open(classifier_name.split(\".\")[0],'rb')\n",
    "    model=pickle.load(model_file)\n",
    "    mapping=pickle.load(map_file)\n",
    "    no=str(str(classifier_name.split(\"Batch\")[1]).split(\".\")[0])\n",
    "    class_file=open(\"Class\"+str(no),'rb')\n",
    "    classes=pickle.load(class_file)\n",
    "    km=K_Means()\n",
    "    km.classifications=classes\n",
    "    km.centroids=model\n",
    "    for i in range(len(labelled_batch)):\n",
    "        current_row=labelled_batch.iloc[i:i+1,:-1]\n",
    "        current_label=labelled_batch.iloc[i:i+1,-1]\n",
    "        pred=km.predict(current_row,mapping)\n",
    "        final_pred=mapping[pred[0]]\n",
    "        if(final_pred==current_label[i]):\n",
    "            s+=1\n",
    "    return((s)/len(labelled_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score,precision_recall_fscore_support,roc_auc_score, accuracy_score, f1_score,recall_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool={}\n",
    "batch_no=1\n",
    "stream_size=int(100)\n",
    "skip=0\n",
    "batch_split = int(0.2 * stream_size)\n",
    "max_c=10\n",
    "acc=[]\n",
    "s=0\n",
    "full_pred=[]\n",
    "full_true=[]\n",
    "#for j in range(0,6):\n",
    "for j in range(0,int(len(data)/stream_size)):\n",
    "    print(\"Batch\",batch_no)\n",
    "    batch = pd.read_csv(dataset_name, nrows = stream_size, skiprows=skip)\n",
    "    \n",
    "    if(j==0):\n",
    "        model_name=\"Batch\"+str(batch_no)+\".sav\"#Initial classifier\n",
    "        map_name=\"Batch\"+str(batch_no)\n",
    "        class_name=\"Class\"+str(batch_no)\n",
    "        model,mapping,classes=create_classifier(batch,Q,batch_split)\n",
    "        pool[model_name]=1\n",
    "        model_file=open(model_name,'wb')\n",
    "        map_file=open(map_name,'wb')\n",
    "        class_file=open(class_name,'wb')\n",
    "        pickle.dump(model,model_file)\n",
    "        pickle.dump(mapping,map_file)\n",
    "        pickle.dump(classes,class_file)\n",
    "    else:\n",
    "        #acc=[]\n",
    "        s=0\n",
    "        labelled_batch=batch.iloc[:batch_split,:]\n",
    "        pred_unlabelled=[]\n",
    "        true_unlabelled=[]\n",
    "        for i in range(len(labelled_batch)):\n",
    "            current_row=labelled_batch.iloc[i:i+1,:-1]\n",
    "            current_label=labelled_batch.iloc[i:i+1,-1]\n",
    "            Keymax = max(pool, key=pool.get)\n",
    "            model_file=open(Keymax,'rb')\n",
    "            map_file=open(Keymax.split(\".\")[0],'rb')\n",
    "            no=str(str(Keymax.split(\"Batch\")[1]).split(\".\")[0])\n",
    "            class_file=open(\"Class\"+str(no),'rb')\n",
    "            model=pickle.load(model_file)\n",
    "            mapping=pickle.load(map_file)\n",
    "            classes=pickle.load(class_file)\n",
    "            km=K_Means()\n",
    "            km.centroids=model\n",
    "            km.classifications=classes\n",
    "            pred=km.predict(current_row,mapping)\n",
    "            final_pred=mapping[pred[0]]\n",
    "            pred_unlabelled.append(final_pred)\n",
    "            true_unlabelled.append(current_label[i])\n",
    "            full_pred.append(final_pred)\n",
    "            full_true.append(current_label[i])\n",
    "            for classifier_name in pool:            \n",
    "                model_file=open(classifier_name,'rb')\n",
    "                map_file=open(classifier_name.split(\".\")[0],'rb')\n",
    "                no=str(str(classifier_name.split(\"Batch\")[1]).split(\".\")[0])\n",
    "                class_file=open(\"Class\"+str(no),'rb')\n",
    "                model=pickle.load(model_file)\n",
    "                mapping=pickle.load(map_file)\n",
    "                classes=pickle.load(class_file)\n",
    "                km=K_Means()\n",
    "                km.centroids=model\n",
    "                km.classifications=classes\n",
    "                pred=km.predict(current_row,mapping)\n",
    "                final_pred=mapping[pred[0]]\n",
    "                if(final_pred!=current_label[i]):\n",
    "                    pool[classifier_name]=pool[classifier_name]*(BETA**1)#Reduce weight otherwise\n",
    "        unlabelled_batch=batch.iloc[batch_split:,:]\n",
    "        for i in range(len(unlabelled_batch)):\n",
    "            current_row=unlabelled_batch.iloc[i:i+1,:-1]\n",
    "            current_label=unlabelled_batch.iloc[i:i+1,-1]\n",
    "            Keymax = max(pool, key=pool.get)\n",
    "            model_file=open(Keymax,'rb')\n",
    "            map_file=open(Keymax.split(\".\")[0],'rb')\n",
    "            no=str(str(Keymax.split(\"Batch\")[1]).split(\".\")[0])\n",
    "            class_file=open(\"Class\"+str(no),'rb')\n",
    "            model=pickle.load(model_file)\n",
    "            mapping=pickle.load(map_file)\n",
    "            classes=pickle.load(class_file)\n",
    "            km=K_Means()\n",
    "            km.centroids=model\n",
    "            km.classifications=classes\n",
    "            pred=km.predict(current_row,mapping)\n",
    "            final_pred=mapping[pred[0]]\n",
    "            pred_unlabelled.append(final_pred)\n",
    "            true_unlabelled.append(current_label[i+batch_split])\n",
    "            full_pred.append(final_pred)\n",
    "            full_true.append(current_label[i+batch_split])\n",
    "            #if(final_pred==current_label[i+batch_split]):\n",
    "              #  s+=1\n",
    "        pool=update_pool(labelled_batch,batch_no,batch)#Update pool\n",
    "        #print(true_unlabelled)\n",
    "        #print(pred_unlabelled)\n",
    "        print(classification_report(true_unlabelled,pred_unlabelled))\n",
    "    #print(acc)\n",
    "    skip += stream_size\n",
    "    batch_no += 1\n",
    "acc1 = accuracy_score(full_true,full_pred)\n",
    "f11 = f1_score(full_true,full_pred,average=None)\n",
    "re1 = recall_score(full_true, full_pred, average = None)\n",
    "pre1 = precision_score(full_true, full_pred, average = None)\n",
    "rep1 = classification_report(full_true, full_pred)\n",
    "final_df = pd.DataFrame(columns = ['Dataset', 'Labelled percent', 'Distance used',  'Accuracy', 'F1', 'Recall', 'Precision', 'Report'])\n",
    "final_df.loc[len(final_df)] = {'Dataset':\"Spambase\", 'Labelled percent':'20',\n",
    "                                    'Distance used':'Euclidean', 'Accuracy': acc1, 'F1': f11,\n",
    "                                    'Recall': re1, 'Precision': pre1, 'Report': rep1} \n",
    "final_df.to_csv('SPASC_results_' + 'Spambase' +  '_ossilh.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
